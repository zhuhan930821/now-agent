import streamlit as st
import os
import sys
from llama_index.core import StorageContext, load_index_from_storage
# å¼ºåˆ¶å°†æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯è®¾ç½®ä¸º utf-8
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
# --- ä¿®æ”¹ç‚¹ 1: å¼•å…¥ Gemini åº“ ---
from llama_index.embeddings.gemini import GeminiEmbedding  # <--- å¿…é¡»æœ‰è¿™ä¸€è¡Œ
from llama_index.llms.gemini import Gemini
from llama_index.core.chat_engine import CondenseQuestionChatEngine
from llama_index.core.memory import ChatMemoryBuffer

# --- é…ç½®éƒ¨åˆ† ---
# å»ºè®®å°† Key æ”¾åœ¨çŽ¯å¢ƒå˜é‡ä¸­ï¼Œæˆ–è€…ç›´æŽ¥å†™åœ¨è¿™é‡Œï¼ˆæ³¨æ„ä¿å¯†ï¼‰
GOOGLE_API_KEY = "AIzaSyAfgLQT8ZsklX5Xxsk7Mdtyo2wLEf6VAj8"

st.set_page_config(page_title="Now Agent", page_icon="ðŸŒ¿", layout="centered")
st.title("ðŸŒ¿ The Power of Now Â· ç–—æ„ˆ Agent (Geminiç‰ˆ)")

# --- æ ¸å¿ƒé€»è¾‘ ---

@st.cache_resource(show_spinner=False)
def load_data_and_index():
    with st.spinner("æ­£åœ¨è¿žæŽ¥å†…åœ¨æ™ºæ…§ (åŠ è½½ Gemini)..."):
        
        # --- ä¿®æ”¹ç‚¹ 2: é…ç½® Gemini ---
        
        # 1. è®¾ç½® LLM (å¤§è„‘)
        # æŽ¨èä½¿ç”¨ "models/gemini-1.5-flash" (é€Ÿåº¦å¿«ï¼Œå…è´¹é¢åº¦é«˜) 
        # æˆ–è€… "models/gemini-1.5-pro" (æ›´èªæ˜Žï¼Œé€‚åˆæ·±å±‚æŽ¨ç†)
        Settings.llm = Gemini(
            model="gemini-3-flash-preview", 
            api_key=GOOGLE_API_KEY,
            temperature=0.3,
            transport="rest"
        )
        
        # 2. è®¾ç½® Embedding (æŠŠä¹¦å˜æˆå‘é‡çš„å·¥å…·)
        # å¿…é¡»è®¾ç½®è¿™ä¸ªï¼Œå¦åˆ™ LlamaIndex ä¼šé»˜è®¤å°è¯•è°ƒç”¨ OpenAI å¯¼è‡´æŠ¥é”™
        Settings.embed_model = GeminiEmbedding(
            model_name="gemini-embedding-001", 
            api_key=GOOGLE_API_KEY
        )
        
        # --- ä¸‹é¢çš„é€»è¾‘ä¸ç”¨å˜ ---
        
        if not os.path.exists("data"):
            os.makedirs("data")
            st.warning("è¯·åœ¨é¡¹ç›®ç›®å½•ä¸‹åˆ›å»º 'data' æ–‡ä»¶å¤¹å¹¶æ”¾å…¥ä¹¦ç±æ–‡ä»¶ã€‚")
            return None

        reader = SimpleDirectoryReader(input_dir="data")
        documents = reader.load_data()
        
        index = VectorStoreIndex.from_documents(documents)
        return index


# --- ç‹¬ç‰¹çš„â€œå¯¼å¸ˆâ€äººæ ¼è®¾å®š ---
SYSTEM_PROMPT = """
ä½ çŽ°åœ¨æ˜¯ Eckhart Tolleï¼ˆåŸƒå…‹å“ˆç‰¹Â·æ‰˜åˆ©ï¼‰ï¼Œã€Šå½“ä¸‹çš„åŠ›é‡ã€‹çš„ä½œè€…ã€‚
ä½ çš„ä»»åŠ¡ä¸æ˜¯ä½œä¸ºä¸€ä¸ªâ€œAIåŠ©æ‰‹â€åŽ»è§£å†³é€»è¾‘é—®é¢˜ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªâ€œçµæ€§å¯¼å¸ˆâ€åŽ»åŒ–è§£ç”¨æˆ·çš„ç—›è‹¦ï¼ˆPain-Bodyï¼‰ã€‚

è¯·éµå¾ªä»¥ä¸‹åŽŸåˆ™ï¼š
1. **æ ¸å¿ƒç«‹åœº**ï¼šæ°¸è¿œå°†ç”¨æˆ·å¼•å¯¼å›žâ€œå½“ä¸‹â€ï¼ˆThe Nowï¼‰ã€‚æŒ‡å‡ºä»–ä»¬çš„ç—›è‹¦æ¥è‡ªäºŽâ€œæ€ç»´è®¤åŒâ€ï¼ˆIdentification with the mindï¼‰æˆ–â€œå¿ƒç†æ—¶é—´â€ï¼ˆPsychological timeï¼‰ã€‚
2. **è¯­æ°”é£Žæ ¼**ï¼šå¹³é™ã€å¯Œæœ‰åŒç†å¿ƒã€æ·±é‚ƒã€ä¸è¯„åˆ¤ã€‚åƒä¸€ä¸ªç¿æ™ºçš„è§‚å¯Ÿè€…ã€‚
3. **å¼•ç”¨åŽŸæ–‡**ï¼šåœ¨å›žç­”æ—¶ï¼Œå¿…é¡»ä¼˜å…ˆæ£€ç´¢å¹¶å¼•ç”¨ã€Šå½“ä¸‹çš„åŠ›é‡ã€‹ä¹¦ä¸­çš„æ¦‚å¿µï¼ˆå¦‚ï¼šç—›è‹¦ä¹‹èº«ã€å°æˆ‘ã€ä¸´åœ¨ã€æœªæ˜¾åŒ–çŠ¶æ€ï¼‰ã€‚
4. **å®žè·µå¯¼å‘**ï¼šä¸è¦åªè®²å¤§é“ç†ï¼Œè¦ç»™å‡ºå…·ä½“çš„ç»ƒä¹ å»ºè®®ï¼ˆä¾‹å¦‚ï¼šå…³æ³¨å‘¼å¸ã€æ„Ÿå—å†…åœ¨èº«ä½“ã€é€šè¿‡è§‚å¯Ÿæƒ…ç»ªæ¥é€šè¿‡å®ƒï¼‰ã€‚
5. **å¤„ç†ç—›è‹¦**ï¼šå½“ç”¨æˆ·è¡¨è¾¾ç—›è‹¦æ—¶ï¼Œä¸è¦è¯•å›¾ç”¨é€»è¾‘åŽ»â€œä¿®è¡¥â€é‚£ä¸ªæ•…äº‹ï¼Œè€Œæ˜¯è®©ä»–ä»¬åŽ»â€œè§‚å¯Ÿâ€é‚£ä¸ªç—›è‹¦ï¼Œä»Žç—›è‹¦ä¸­åˆ†ç¦»å‡ºæ¥ã€‚

å¦‚æžœç”¨æˆ·é—®éžçµæ€§é—®é¢˜ï¼Œè¯·ç¤¼è²Œåœ°å°†è¯é¢˜å¼•å›žåˆ°æ„è¯†å’Œå½“ä¸‹çš„å±‚é¢ã€‚
"""

# --- åˆå§‹åŒ– ---
index = load_data_and_index()

if index:
    # åˆå§‹åŒ–èŠå¤©å¼•æ“Ž
    if "chat_engine" not in st.session_state:
        # ä½¿ç”¨ context æ¨¡å¼ï¼Œè®© AI æ—¢èƒ½æŸ¥ä¹¦ï¼Œåˆæœ‰è®°å¿†
        memory = ChatMemoryBuffer.from_defaults(token_limit=3000)
        st.session_state.chat_engine = index.as_chat_engine(
            chat_mode="context",
            memory=memory,
            system_prompt=SYSTEM_PROMPT,
            verbose=False
        )

    # --- èŠå¤©ç•Œé¢ ---
    
    # æ˜¾ç¤ºåŽ†å²æ¶ˆæ¯
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("æ­¤åˆ»ï¼Œä½ æ„Ÿå—åˆ°äº†ä»€ä¹ˆï¼Ÿ"):
        # 1. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. ç”Ÿæˆå›žç­”
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # è°ƒç”¨ RAG å¼•æ“Ž
            response = st.session_state.chat_engine.stream_chat(prompt)
            
            for token in response.response_gen:
                full_response += token
                message_placeholder.markdown(full_response + "â–Œ")
            
            message_placeholder.markdown(full_response)
            
            # å¯é€‰ï¼šæ˜¾ç¤ºå®ƒå‚è€ƒäº†ä¹¦é‡Œçš„å“ªä¸€æ®µï¼ˆè°ƒè¯•ç”¨ï¼‰
            with st.expander("æŸ¥çœ‹çµæ„Ÿæ¥æº (Source Context)"):
                 st.write(response.source_nodes)

        # 3. ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
        st.session_state.messages.append({"role": "assistant", "content": full_response})

else:
    st.write("ç­‰å¾…æ•°æ®åŠ è½½...")